united states general-accounting-office gao october 2002 external version 1 assessing the reliability of computer-processed-data a contents preface section 1 : introduction section 2 : understanding-data-reliability assessment page i gao-03-273g assessing-reliabillity 23 section 8 : conducting tracing to and from source-documents 24 using advanced electronic-testing 25 reviewing selected-system-controls 26 using-data of undetermined-reliability 27 28 section 9 : making the sufficiently-reliable data 29 not sufficiently-reliable data 29 data of undetermined-reliability 30 31 section 10 : including sufficiently-reliable data 31 not sufficiently-reliable data 31 in the report data of undetermined-reliability 32 glossary of technical-terms figure 1 : figures figure 2 : figure 3 : figure 4 : figure 5 : figure 6 : figure 7 : factors to consider in making the decision on using the data 1 decision-process for determining if a data-reliability-assessment is required 7-data-reliability assessment-process 13 the first steps of the assessment 14 the preliminary-assessment 19 choosing and conducting additional-work 23 making the final-assessment 28 preface computer-processed-data , often from external-sources , increasingly underpin-audit-reports , including-evaluations ( performance-audits ) and financial-audits . therefore , the reliability of such data has become more and more important . historically , computer-processed-data have been treated as unique evidence . however , these data are simply one form of evidence relied on , although they may require more technical-assessment than other forms of evidence . in addition , the very nature of the information-system creating the data allows opportunities for errors to be introduced by many people . this guidance is intended to demystify the assessment of computerprocessed-data . it supplements gao 's `` yellow-book '' ( government-auditing-standards , 1994 revision ) , which defines the generally accepted-government-auditing standards ( gagas ) , and replaces the earlier gao guidance , assessing the reliability of computer-processed-data ( gao/op-8.1.3 , sept. 1990 ) . for all types of evidence , various-tests are used-sufficiency , competence , and relevance-to assess whether the evidence-standard is met . you probably have been using these tests for years and have become quite-proficient at them . but because assessing computer-processed-data requires more technical-tests , it may appear that such data are subject to a higher standard of testing than other evidence . that is not the case . for example , many of the same tests of sufficiency and relevance are applied to other types of evidence . but in assessing computer-processed-data , the focus is on one test in the evidence-standard-competence-which includes validity and reliability . reliability , in turn , includes the completeness and accuracy of the data . this guidance , therefore , provides a flexible , risk-based framework for data-reliability-assessments that can be geared to the specific-circumstances of each engagement . the framework also provides a structure for planning and reporting , facilitates bringing the right-mix of skills to each engagement , and ensures timely-management-buy-in on assessment-strategies . the framework is built on • making-use of all existing-information about the data , • performing at least a minimal-level of data testing , • doing only the amount of work necessary to determine whether the data are reliable-enough for our purposes , • maximizing professional-judgment , and • bringing the appropriate-people , including-management , to the table at key-decision-points . the ultimate-goal of the data-reliability-assessment is to determine whether you can use the data for your intended-purposes . this guidance is designed to help you make an appropriate , defensible-assessment in the most efficient-manner . with any related-questions , call barbara johnson , focal-point for data-reliability-issues , at ( 202 ) 512-3663 , or barry seltser , the acting director of gao 's center for design , methods , and analysis , at ( 202 ) 512-3234 . nancy kingsbury managing director , applied-research and methods-section 1 : introduction this guidance explains what data-reliability-means and provides a framework for assessing the reliability of computer-processed-data . it begins with the steps in a preliminary-assessment , which , in many cases , may be all you need to do to assess reliability . this guidance also helps you decide whether you should follow up the preliminary-assessment with additional-work . if so , it explains the steps in a final-assessment and the actions to take , depending on the results of your additional-work . the ultimate-goal in determining-data-reliability is to make the following-decision : for our engagement , can we use the data to answer the research-question ? see figure 1 for an overview of the factors that help to inform that decision . not all of these factors may be necessary for all engagements . figure 1 : factors to consider in making the decision on using the data source : gao . in addition , this guidance discusses suggested language-appropriate under different-circumstances-for-reporting the results of your assessment . finally , it provides detailed-descriptions of all the stages of the assessment , as well as a glossary of technical-terms used ( see p. 33 ) . an on-line-version of this guidance , which will include tools that may help you in assessing-reliability , is currently being developed . the overall process is illustrated in figures 2 ( p. 7 ) and 3 ( p. 13 ) . section 2 : understanding-data-reliability data-reliability refers to the accuracy and completeness of computerprocessed-data , given the intended-purposes for use . computer-processed-data include data ( 1 ) entered into a computer-system and ( 2 ) resulting from computer-processing . computer-processed-data can vary in form-from electronic-files to tables in published reports . the definition of computerprocessed-data is therefore-broad . in this guidance , the term-data always refers to computer-processed-data . the `` yellow-book '' requires that a data-reliability-assessment be performed for all data used as support for engagement-findings , conclusions , or recommendations.1 this guidance will help you to design a data-reliability-assessment appropriate for the purposes of the engagement and then to evaluate the results of the assessment . data are reliable when they are ( 1 ) complete ( they contain all of the data-elements and records needed for the engagement ) 2 and ( 2 ) accurate ( they reflect the data entered at the source or , if available , in the source-documents ) . a subcategory of accuracy is consistency . consistency refers to the need to obtain and use data that are clear and well-defined enough to yield similar-results in similar-analyses . for example , if data are entered at multiple-sites , inconsistent-interpretation of data-rules can lead to data that , taken as a whole , are unreliable . reliability also means that for any computer-processing of the data-elements used , the results are reasonably-complete and accurate , meet your intended-purposes , and are not subject to inappropriate-alteration . assessments of reliability should be made in the broader context of the particular-characteristics of the engagement and the risk associated with the possibility of using-data of insufficient reliability . reliability does not mean that computer-processed-data are error-free . errors are considered acceptable under these circumstances : you have assessed the associated risk and found the errors are not significant enough to cause a reasonable-person , aware of the errors , to doubt a finding , conclusion , or recommendation based on the data . 1u.s . general-accounting-office , government-auditing-standards , gao/ogc-94-4 ( washington , d.c. : june 1994 ) , pp . 62-87 . 2a data-element is a unit of information with definable-parameters ( for example , a social-security-number ) , sometimes referred to as a data-variable or data-field . page 3 gao-03-273g assessing-reliability while this guidance focuses only on the reliability of data in terms of accuracy and completeness , other data-quality considerations are just as important . in particular , you should also consider the validity of data . validity ( as used here ) refers to whether the data actually represent what you think is being measured . for example , if a data-field is named `` annual-evaluation score , '' is this an appropriate-measure of a person 's job performance ? considerations of data-validity and reliability-issues should be addressed early in the engagement , and appropriate technical-specialists-such as data-analysts , statisticians , or information-technology-specialists-should be consulted . section 3 : deciding if a data-reliability-assessment is necessary to decide if a data-reliability-assessment is necessary , you should consider certain-conditions . the engagement-type and planned use of the data-help to determine when you should assess data-reliability . see figure 2 for an illustration of the decision-process that you should use . figure 2 : decision-process for determining if a data-reliability-assessment is required source : gao . conditions requiring a data-reliability-assessment you should assess reliability if the data to be analyzed are intended to support the engagement-findings , conclusions , or recommendations . keep in mind that a finding may include only a description of the condition , as in a purely descriptive-report . in the audit-plan for the engagement , you should include a brief-discussion of how you plan to assess data-reliability , as well as any limitations that may exist due to shortcomings in the data . conditions not requiring a data-reliability-assessment you do not need to assess reliability if the data are used ( 1 ) only as background-information or ( 2 ) in documents without findings , conclusions , or recommendations . background-information generally sets the stage for reporting the results of an engagement or provides information that puts the results in proper-context . such information could be the size of the program or activity you are reviewing , for example . when you gather background or other data , ensure that they are from the best-available source ( s ) . when you present the data , cite the source ( s ) and state that the data were not assessed . sometimes , as a best practice , however , you may want to do some assessment of background-data . your judgment of the data's importance and the reliability of the source , as well as other engagement-factors , can help you determine the extent of such an assessment . finally , for financial-audits and information-system-reviews , you should not follow this guidance in assessing-data-reliability . for financial-audits , which include financial-statement and financial-related-audits , you should follow the gao/pcie financial-audit manual ( fam ) and the federal-information system-controls audit manual ( fiscam ) . in an information-system-review , all controls in a computer-system , for the full-range of application functions and products , are assessed and tested . such a review includes ( 1 ) examining the general and application-controls of a computer system,3 ( 2 ) testing whether those controls are being complied with , and ( 3 ) testing-data produced by the system.4 to design such a review , appropriate to the research-question , seek assistance from information-technology-specialists . 3general-controls refers to the structure , policies , and procedures-which apply to all or a large-segment of an organization 's information systems-that-help to ensure proper operation , data-integrity , and security . application-controls refers to the structure , policies , and procedures that apply to individual-application-systems , such as inventory or payroll . 4guidance for carrying out reviews of general and application-controls is provided in the u.s. general-accounting-office , federal-information system-controls audit manual , gao/aimd-12.19.6 ( washington , d.c. : jan. 1999 ) . section 4 : performing a data-reliability-assessment timing the assessment to perform a data-reliability-assessment , you need to decide on the timing-when to perform the assessment-and how to document it . a data-reliability-assessment should be performed as early as possible in the engagement-process , preferably during the design-phase . the audit-plan should reflect data-reliability-issues and any additional-steps that still need to be performed to assess the reliability of critical-data . the engagement-team generally should not finalize the audit-plan or issue a commitment-letter until it has done initial-testing and reviewed existing-information about the data and the system that produces the data . in addition , the team should not commit to making-conclusions or recommendations based on the data unless the team expects to be satisfied with the data-reliability . documenting the assessment all work performed as part of the data-reliability-assessment should be documented and included in the engagement-workpapers . this includes all testing , information-review , and interviews related to data-reliability . in addition , decisions made during the assessment , including the final-assessment of whether the data are sufficiently-reliable for the purposes of the engagement , should be summarized and included with the workpapers . these workpapers should be ( 1 ) clear about what steps the team took and what conclusions they reached and ( 2 ) reviewed by staff with appropriate-skills or , if needed , technical-specialists . section 5 : viewing the entire-assessment-process the ultimate-goal of the data-reliability-assessment is to determine whether you can use the data to answer the research-question . the assessment should be performed only for those portions of the data that are relevant to the engagement . the extensiveness of the assessment is driven by • the expected-significance of the data to the final-report , • the anticipated-risk-level of using the data , and • the strength or weakness of any corroborating-evidence . therefore , the specific-assessment-process should take into account these factors along with what is learned during the initial-stage of the assessment . the process is likely to be different for each engagement . the overall-framework of the process for data-reliability-assessment is shown in figure 3 . the framework identifies several key-stages in the assessment , as well as actions and decisions expected as you move through the process . the framework allows you to identify the appropriate-mix of assessment-steps to fit the particular-needs of your engagement . in most cases , all of the elements in figure 3 would not be necessary in completing the assessment . specific-actions for each stage are discussed in sections 6-10 . figure 3 : data-reliability-assessment process-source : gao . section 6 : taking the first steps reviewing existing-information the data-reliability-process begins with two relatively-simple steps . these steps provide the basis for making a preliminary-assessment of data-reliability : ( 1 ) a review of related-information and ( 2 ) initial-testing ( see figure 4 ) . in some situations , you may have an extremely short-time-frame for the engagement ; this section also provides some advice for this situation . the time required to review related-information and perform initial-testing will vary , depending on the engagement and the amount of risk involved . as discussed in section 4 , these steps should take place early in the engagement and include the team-members , as well as appropriate technical-staff . figure 4 : the first steps of the assessment source : gao . the first step-a-review of existing-information-helps you to determine what is already known about the data and the computer-processing . the related-information you collect can indicate both the accuracy and completeness of the entry and processing of the data , as well as how data-integrity is maintained . this information can be in the form of reports , studies , or interviews with individuals who are knowledgeable about the data and the system . sources for related-information include gao , the agency under review , and others . gao gao may already have related-information in reports . those from fiscal year 1995 to the present are available via gao's internet-site . this site also provides other useful-information : for example , as part of the annual-governmentwide consolidated financial-audit , gao 's information technology team is involved with reporting on the effectiveness of controls for financial information-systems at 24 major federal-agencies . agency under review officials of the agency or entity under review are aware of evaluations of their computer-data or systems and usually can direct you to both . however , keep in mind that information from agency-officials may be biased . consider asking appropriate technical-specialists to help in evaluating this information . agency-information includes inspector-general reports , federal-managers ' financial-integrity act reports , government-performance and results act ( gpra ) plans and reports , clinger-cohen act reports , and chief-information-officer reports . ( some of this information can be found in agency homepages on the web . ) others other organizations and users of the data may be sources of relevant-information . to help you identify these sources , you can use a variety of databases and other research-tools , which include the congressional research service public-policy literature abstracts and organizations ' web sites . performing initial-testing the second step-initial-testing-can be done by applying logical tests to electronic-data-files or hard-copy-reports . for electronic-data , you use computer-programs to test all entries of key-data-elements in the entire data file.5 keep in mind that you only test those data-elements you plan to use for the engagement . you will find that testing with computer-programs often takes less than a day , depending on the complexity of the file . for 5 though an in-depth-discussion of quality-assurance-practices to be used in electronic-testing and analyses is beyond the scope of this guidance , it is important to perform appropriate-checks to ensure that you have obtained the correct-file . all too often , analysts receive an incorrect-file ( an early-version or an incomplete-file ) . appropriate-steps would include counting-records and comparing-totals with the responsible-agency or entity . page 15 gao-03-273g assessing-reliability dealing with short-time-frames hard-copy or summarized data-provided by the audited-entity or retrieved from the internet-you can ask for the electronic-data-file used to create the hard-copy or summarized-data . if you are unable to obtain electronic-data , use the hard-copy or summarized-data and , to the extent possible , manually apply the tests to all instances of key-data-elements or , if the report or summary is voluminous , to a sample of them . whether you have an electronic-data-file or a hard-copy-report or summary , you apply the same types of tests to the data . these can include testing for • missing-data , either entire-records or values of key-data-elements ; • the relationship of one data-element to another ; • values outside of a designated range ; and • dates outside valid-time-frames or in an illogical progression . be sure to keep a log of your testing for inclusion in the engagement-workpapers . in some instances , the engagement may have a time frame that is too short for a complete preliminary-assessment , for example , a request for testimony in 2 weeks . however , given that all engagements are a function of time , as well as scope and resources , limitations in one require balancing the others . despite a short-time-frame , you may have time to review existing-information and carry out testing of data that are critical for answering a research-question , for example : you can question knowledgeable-agency-staff about data-reliability or review existing gao or inspector-general reports to quickly gather information about data-reliability-issues . in addition , electronic-testing of critical-data-elements for obvious-errors of completeness and accuracy can generally be done in a short period of time on all but the most complicated or immense-files . from that review and testing , you will be able to make a more informed determination about whether the data are sufficiently-reliable to use for the purposes of the engagement . ( see sections 7 and 8 for the actions to take , depending on your determination . ) section 7 : making the preliminary-assessment factors to consider in the assessment the preliminary-assessment is the first decision-point in the assessment-process , including the consideration of multiple factors , a determination of the sufficiency of the data-reliability with what is known at this point , and a decision about whether further work is required . you will decide whether the data are sufficiently-reliable for the purposes of the engagement , not sufficiently-reliable , or as yet undetermined . keep in mind that you are not attesting to the overall-reliability of the data or database . you are only determining the reliability of the data as needed to support the findings , conclusions , or recommendations of the engagement . as you gather information and make your judgments , consult appropriate technical-specialists for assistance . to make the preliminary-assessment of the sufficiency of the data-reliability for the engagement , you should consider all factors related to aspects of the engagement , as well as assessment work performed to this point . as shown in figure 5 , these factors include • the expected-significance of the data in the final-report , • corroborating-evidence , • level of risk , and • the results of initial-assessment-work . figure 5 : the preliminary-assessment source : gao . expected-significance of in making the preliminary-assessment , consider the data in the context of the final-report : will the engagement-team depend on the data alone to the data in the final-report answer a research-question ? will the data be summarized or will detailed-information be required ? is it important to have precise data , making-magnitude of errors an issue ? corroborating-evidence you should consider the extent to which corroborating-evidence is likely to exist and will independently support your findings , conclusions , or recommendations . corroborating-evidence is independent-evidence that supports information in the database . such evidence , if available , can be found in the form of alternative-databases or expert-views . it is unique to each engagement , and its strength-persuasiveness-varies . for help in deciding the strength or weakness of corroborating-evidence , consider the extent to which the corroborating-evidence • is consistent with the `` yellow-book '' standards of evidence-sufficiency , competence , and relevance ; • provides crucial-support ; level of risk • is drawn from different types of sources-testimonial , documentary , physical , or analytical ; and • is independent of other sources . risk is the likelihood that using-data of questionable reliability could have significant negative-consequences on the decisions of policymakers and others . to do a risk-assessment , consider the following-risk-conditions : • the data could be used to influence legislation , policy , or a program that could have significant impact . • the data could be used for significant-decisions by individuals or organizations with an interest in the subject . • the data will be the basis for numbers that are likely to be widely quoted , for example , `` in 1999 , the united states owed the united nations about $ 1.3 billion for the regular and peacekeeping budgets . '' • the engagement is concerned with a sensitive or controversial-subject . • the engagement has external-stakeholders who have taken positions on the subject . • the overall-engagement-risk is medium or high . • the engagement has unique-factors that strongly increase risk . bear in mind that any one of the conditions may have more importance than another , depending on the engagement . results of initial-assessment-work at this point , as shown in figure 5 ( p. 19 ) , the team will already have performed the initial-stage of the data-reliability-assessment . they should have the results from the ( 1 ) review of all available existing-information about the data and the system that produced them and ( 2 ) initial-testing of the critical-data-elements . these results should be appropriately documented and reviewed before the team enters into the decision-making-phase of the preliminary-assessment . because the results will , in whole or in part , provide the evidence that the data are sufficiently reliable-and therefore competent-enough-or not sufficiently-reliable for the purposes outcomes to consider in the assessment of the engagement , the workpapers should include documentation of the process and results . the results of your combined judgments of the strength of corroborating-evidence and degree of risk suggest different assessments . if the corroborating-evidence is strong and the risk is low , the data are more likely to be considered sufficiently-reliable for your purposes . if the corroborating-evidence is weak and the risk is high , the data are more likely to be considered not sufficiently-reliable for your purposes . the overall-assessment is a judgment-call , which should be made in the context of discussion with team-management and technical-specialists . the preliminary-assessment categorizes the data as sufficiently-reliable , not sufficiently-reliable , or of undetermined-reliability . each category has implications for the next-steps of the data-reliability-assessment . when to assess data as sufficiently-reliable for engagement-purposes you can assess the data as sufficiently-reliable for engagement-purposes when you conclude the following : both the review of related-information and the initial-testing provide assurance that ( 1 ) the likelihood of significant-errors or incompleteness is minimal and ( 2 ) the use of the data would not lead to an incorrect or unintentional-message . you could have some problems or uncertainties about the data , but they would be minor , given the research-question and intended-use of the data . when the preliminary-assessment indicates that the data are sufficiently-reliable , use the data . when to assess data as not sufficiently-reliable for engagement-purposes you can assess the data as not sufficiently-reliable for engagement-purposes when you conclude the following : the review of related-information or initial-testing indicates that ( 1 ) significant-errors or incompleteness exist in some or all of the key-data-elements and ( 2 ) using the data would probably lead to an incorrect or unintentional-message . when the preliminary-assessment indicates that the data are not sufficiently-reliable , you should seek evidence from other sources , including ( 1 ) alternative computerized data-the-reliability of which you should also assess-or ( 2 ) original-data in the form of surveys , case-studies , or expert-interviews . when to assess data as of undetermined-reliability and consider additional-work you should coordinate with the requester if seeking-evidence from other sources does not result in a source of sufficiently-reliable data . inform the requester that such data , needed to respond to the request , are unavailable . reach an agreement with the requester to • redefine the research-questions to eliminate the need to use the data , • end the engagement , or • use the data with appropriate-disclaimers . remember that you-not the requester-are responsible for deciding what data to use . if you decide you must use data that you have determined are not sufficiently-reliable for the purposes of the engagement , make the limitations of the data clear , so that incorrect or unintentional-conclusions will not be drawn . finally , given that the data you assessed have serious-reliability weaknesses , you should include this finding in the report and recommend that the agency take corrective-action . you can assess the data as of undetermined-reliability when you conclude one of the following : • the review of some of the related-information or initial-testing raises questions about the data 's reliability . • the related-information or initial-testing provides too little-information to judge reliability . • the time or resource-constraints limit the extent of the examination of related-information or initial-testing . when the preliminary-assessment indicates that the reliability of the data is undetermined , consider doing additional-work to determine reliability . section 8 provides guidance on the types of additional-work to consider , as well as suggestions if no additional-work is feasible . section 8 : conducting additional-work when you have determined ( through the preliminary-assessment ) that the data are of undetermined-reliability , consider conducting additional-work ( see figure 6 ) . a range of additional-steps to further determine data-reliability includes tracing to and from source-documents , using advanced electronic-testing , and reviewing selected-system-controls . the mix depends on what weaknesses you identified in the preliminary-assessment and the circumstances specific to your engagement , such as risk-level and corroborating-evidence , as well as other factors . focus particularly on those aspects of the data that pose the greatest potential-risk for your engagement . you should get help from appropriate technical-specialists to discuss whether additional-work is required and to carry out any part of the additional-reliability-assessment . figure 6 : choosing and conducting additional-work source : gao . tracing to and from source-documents tracing a sample of data-records to source-documents helps you to determine whether the computer-data accurately and completely reflect these documents . in deciding what and how to trace , consider the relative-risks to the engagement of overstating or understating the conclusions drawn from the data , for example : on the one hand , if you are particularly-concerned that questionable-cases might not have been entered into the computer-system and that as a result , the degree of compliance may be overstated , you should consider tracing from source-documents to the database . on the other hand , if you are more concerned that ineligible-cases have been included in the database and that as a result , the potential-problems may be understated , you should consider tracing from the database back to source-documents . the reason to trace only a sample is because sampling saves time and cost . to be useful , however , the sample should be random and large enough to estimate the error-rate within reasonable-levels of precision . tracing a random-sample will provide the error-rate and the magnitude of errors for the entire-data-file . it is this error-rate that helps you to determine the data-reliability . generally , every data-file will have some degree of error ( see example 1 for error-rate and example 2 for magnitude of errors ) . consult statisticians to assist you in selecting the sampling-method most suited to the engagement . example 1 : according to a random-sample , 10 percent of the data-records have incorrect-dates . however , the dates may be off by an average of only 3 days . depending on what the data are used for , 3 days may not compromise reliability . example 2 : the value of a data-element was incorrectly entered as $ 100,000 , rather than $ 1,000,000 . the documentation of the database shows that the acceptable-range for this data-element is between $ 100 and $ 5,000,000 . therefore , the electronic-testing done in the initial-testing-phase would have confirmed that the value of $ 100,000 fell within that range . in this case , the error could be caught , not by electronic-testing , but only by tracing the data to source-documents . tracing to source-documents consider tracing to source-documents when ( 1 ) the source-documents are available relatively easily or ( 2 ) the possible magnitude of errors is especially-critical . to trace a sample to source-documents , match the entered data with the corresponding-data in the source-documents . but in attempting to trace entered data back to source-documents , several-problems can arise : source-documents may not be available because they were destroyed , were never created , or are not centrally located . several-options exist if source-documents are not available . for those documents never created-for-example , when data may be based on electronic-submissions-use-interviews to obtain related-information , any corroborating-evidence obtained earlier , or a review of the adequacy of system-controls . tracing from source-documents consider tracing from source-documents , instead of or in addition to tracing a sample to source-documents , when you have concerns that the data are not complete . to trace a sample from source-documents , match the source-documents with the entered data . such tracing may be appropriate to determine whether all data are completely entered . however , if source-documents were never created or are now missing , you can not identify the missing-data . using advanced electronic-testing advanced electronic-testing goes beyond the basic electronic-testing that you did in initial-testing ( see section 5 ) . it generally requires specialized computer-programs to test for specific-conditions in the data . such testing can be particularly-helpful in determining the accuracy and completeness of processing by the application-system that produced the data . consider using advanced electronic-testing for • following up on troubling-aspects of the data-such as extremely high values associated with a certain geographic location-found in initial-testing or while analyzing the data ; reviewing selected-system-controls • testing-relationships-cross-tabulation-between-data elements , such as whether data-elements follow a skip-pattern from a questionnaire ; and • verifying that computer-processing is accurate and complete , such as testing a formula used in generating specific-data elements . depending on what will be tested , this testing can require a range of programming-skills-from creating-cross-tabulations on related data-elements to duplicating an intricate automated-process with more advanced-programming-techniques . consult appropriate technical-specialists , as needed . your review of selected system controls-the underlying structures and processes of the computer in which the data are maintained-can provide some assurance that the data are sufficiently-reliable . examples of system-controls are limits on access to the system and edit-checks on data entered into the system . controls can reduce , to an acceptable-level , the risk that a significant-mistake could occur and remain undetected and uncorrected . limit the review to evaluating the specific-controls that can most directly affect the reliability of the data in question . choose areas for review on the basis of what is known about the system . sometimes , you identify potential-system-control problems in the initial-steps of the assessment . other times , you learn during the preliminary-assessment that source-documents are not readily-available . therefore , a review of selected-system-controls is the best method to determine if data were entered reliably . if needed , consult information-system-auditors for help in evaluating general and application-controls . using what you know about the system , concentrate on evaluating the controls that most directly affect the data . these controls will usually include ( 1 ) certain-general-controls , such as logical access and control of changes to the data , and ( 2 ) the application-controls that help to ensure that the data are accurate and complete , as well as authorized . the steps for reviewing selected-system-controls are • gain a detailed-understanding of the system as it relates to the data and • identify and assess the application and general-controls that are critical to ensuring the reliability of the data required for the engagement . in some situations , it may not be feasible to perform any additional-work , using-data of for example , when ( 1 ) given a short-time-frame ( too short for a complete-assessment ) , ( 2 ) original-computer-files have been deleted , or ( 3 ) access to reliability needed documents is unavailable . see section 9 for how to proceed . section 9 : making the final-assessment during the final-assessment , you should consider the results of all your previous-work to determine whether , for your intended-use , the data are sufficiently-reliable , not sufficiently-reliable , or still undetermined . again , remember that you are not attesting to the reliability of the data or database . you are only determining the sufficiency of the reliability of the data for your intended-use . the final-assessment will help you decide what actions to take ( see figure 7 ) . figure 7 : making the final-assessment source : gao . the following are some considerations to help you decide whether you can use the data : • the corroborating-evidence is strong . • the degree of risk is low . • the results of additional-assessment ( 1 ) answered-issues raised in the preliminary-assessment and ( 2 ) did not raise any new questions . • the error-rate , in tracing to or from source-documents , did not compromise reliability . in making this assessment , you should consult with appropriate technical-specialists . you can consider the data sufficiently-reliable when you conclude the following : on the basis-of-the additional-work , as well as the initial-assessment-work , using the data would not weaken the analysis nor lead to an incorrect or unintentional-message . you could have some problems or uncertainties about the data , but they would be minor , given the research-question and intended-use of the data . when your final-assessment indicates that the data are reliable , use the data . sufficiently-reliable data not sufficiently-reliable data you can consider the data to be not sufficiently-reliable when you conclude the following : on the basis of information drawn from the additional-assessment , as well as the preliminary-assessment , ( 1 ) using the data would most likely-lead to an incorrect or unintentional-message and ( 2 ) the data have significant or potentially-significant limitations , given the research-question and intended-use of the data . when you determine that the data are not sufficiently-reliable , you should inform the requester that sufficiently-reliable data , needed to respond to the request , are unavailable . remember that you-not the requester-are responsible for deciding what data to use . although the requester may want information based on insufficiently reliable-data , you are responsible for ensuring that data are used appropriately to respond to the requester . if you decide to use the data for the report , make the limitations of the data clear , so that incorrect or unintentional-conclusions will not be arrived at . appropriate-team-management should be consulted before you agree to use data that are not sufficiently-reliable . finally , given that the data you assessed have serious-reliability weaknesses , you should include this finding in the report and recommend that the agency take corrective-action . data of undetermined-reliability you can consider the data to be of undetermined-reliability when you conclude the following : on the basis-of-the information drawn from any additional-work , as well as the preliminary-assessment , ( 1 ) use of the data could lead to a incorrect or unintentional-message and ( 2 ) the data have significant or potentially-significant limitations , given the research-question and the intended-use . you can consider the data to be of undetermined-reliability if specific-factors-such as short-time-frames , the deletion of original-computer-files , and the lack of access to needed documents-are-present . if you decide to use the data , make the limitations of the data clear , so that incorrect or unintentional-conclusions will not be arrived at . as noted above in the case of not sufficiently-reliable data , when you determine that the data are of undetermined-reliability , you should inform the requester-if-appropriate-that sufficiently-reliable data , needed to respond to the request , are unavailable . remember that you-not the requester-are responsible for deciding what data to use . although the requester may want information based on data of undetermined-reliability , you are responsible for ensuring that appropriate-data are used to respond to the requester . if you decide to use the data in your report , make the limitations clear , so that incorrect or unintentional-conclusions will not be arrived at . appropriate-team-management should be consulted before you agree to use data of undetermined-reliability . section 10 : including appropriate-language in the report sufficiently-reliable data in the report , you should include a statement in the methodology-section about conformance to generally accepted-government-auditing standards ( gagas ) . these standards refer to how you did your work , not how reliable the data are . therefore , you are conforming to gagas as long as , in reporting , you discuss what you did to assess the data ; disclose any data-concerns ; and reach a judgment about the reliability of the data for use in the report . furthermore , in the methodology-section , include a discussion of your assessment of data-reliability and the basis for this assessment . the language in this discussion will vary , depending on whether the data are sufficiently-reliable , not sufficiently-reliable , or of undetermined-reliability . in addition , you may need to discuss the reliability of the data in other sections of the report . whether you do so depends on the importance of the data to the message . present your basis for assessing the data as sufficiently-reliable , given the research-questions and intended-use of the data . this presentation includes ( 1 ) noting what kind of assessment you relied on , ( 2 ) explaining the steps in the assessment , and ( 3 ) disclosing any data-limitations . such disclosure includes • telling why using the data would not lead to an incorrect or unintentional-message , • explaining how limitations could affect any expansion of the message , and • pointing out that any data-limitations are minor in the context of the engagement . present your basis for assessing the data as not sufficiently-reliable , given not sufficiently the research-questions and intended-use of the data . this presentation should include what kind of assessment you relied on , with an explanation of the steps in the assessment . data of undetermined-reliability in this explanation , ( 1 ) describe the problems with the data , as well as why using the data would probably lead to an incorrect or unintentional-message , and ( 2 ) state that the data-problems are significant or potentially-significant . in addition , if the report contains a conclusion or recommendation supported by evidence other than these data , state that fact . finally , if the data you assessed are not sufficiently-reliable , you should include this finding in the report and recommend that the audited-entity take corrective-action . present your basis for assessing the reliability of the data as undetermined . include such factors as short-time-frames , the deletion of original-computer-files , and the lack of access to needed documents . explain the reasonableness of using the data , for example : these are the only available-data on the subject ; the data are widely used by outside-experts or policymakers ; or the data are supported by credible corroborating-evidence . in addition , make the limitations of the data clear , so that incorrect or unintentional-conclusions will not be drawn from the data . for example , indicate how the use of these data could lead to an incorrect or unintentional-message . finally , if the report contains a conclusion or recommendation supported by evidence other than these data , state that fact . glossary of technical-terms accuracy . freedom from error in the data . completeness . the inclusion of all necessary-parts or elements . database . a collection of related data-files ( for example , questionnaire-responses from several different-groups of people , with each group 's identity maintained . ) data-element . an individual-piece of information that has definable-parameters , sometimes referred to as variables or fields ( for example , the response to any question in a questionnaire ) . data-file . a collection of related data-records , also referred to as a data-set ( for example , the collected questionnaire-responses from a group of people ) . data-record . a collection of related data-elements that relate to a specific-event , transaction , or occurrence ( for example , questionnaire-responses about one individual-such as age , sex , and marital-status ) . source-document . information that is the basis for entry of data into a computer . gao 's mission the general-accounting-office , the investigative-arm of congress , exists to support congress in meeting its constitutional-responsibilities and to help improve the performance and accountability of the federal government for the american people . gao examines the use of public-funds ; evaluates federal-programs and policies ; and provides analyses , recommendations , and other assistance to help congress make informed-oversight , policy , and funding-decisions . gao 's commitment to good-government is reflected in its core-values of accountability , integrity , and reliability . obtaining-copies of gao reports and testimony the fastest and easiest way to obtain copies of gao documents at no cost is through the internet . gao 's web site ( www.gao.gov ) contains abstracts and fulltext-files of current-reports and testimony and an expanding-archive of older products . the web site features a search engine to help you locate documents using-key words and phrases . you can print these documents in their entirety , including-charts and other graphics . each day , gao issues a list of newly released reports , testimony , and correspondence . gao posts this list , known as '' today 's reports , '' on its web site daily . the list contains links to the full-text-document-files . to have gao e-mail this list to you every afternoon , go to www.gao.gov and select `` subscribe to daily-e-mail-alert for newly released products '' under the gao reports heading . order by mail or phone the first copy of each printed report is free . additional-copies are $ 2 each . a check or money-order should be made out to the superintendent of documents . gao also accepts visa and mastercard . orders for 100 or more copies mailed to a single-address are discounted 25 percent . orders should be sent to : u.s. general-accounting-office 441 g street nw , room lm washington , d.c. 20548 to order by phone : voice : ( 202 ) 512-6000 tdd : ( 202 ) 512-2537 fax : ( 202 ) 512-6061 contact : to report fraud , web site : www.gao.gov/fraudnet/fraudnet.htm e-mail : fraudnet @ gao.gov federal-programs automated answering-system : ( 800 ) 424-5454 or ( 202 ) 512-7470 jeff nelligan , managing director , nelliganj @ gao.gov ( 202 ) 512-4800 public-affairs u.s. generalaccounting office , 441 g street nw , room 7149 washington , d.c. 20548 presorted standard postage & fees paid gao permit no . gi00 united states general-accounting-office washington , d.c. 20548-0001 official business penalty for private-use $ 300 address service requested 